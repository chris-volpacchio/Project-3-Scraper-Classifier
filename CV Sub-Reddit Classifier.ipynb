{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "# <u>*Program–Rating Classifier*</u>\n",
    "#### The following code demonstrates the development and refinement of a binary text classifier designed to read and ingest data from two sub-reddit threads of popular television programs. I will be using the Pushshift api to navigate throughout Reddit.com and the two forementioned sub-reddits. The penultimate is to establish a classifier that will group posts by identifying key terms associated with each topic and then looking for them in the text. Our final step is to have a classifier that is ready to be tested and strengthened through user feedback.\n",
    "\n",
    "* [Subreddit Scraping](#scraping)\n",
    "* [Data Clean](#cleaning)\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "##########\n",
    "\n",
    "#VEctorize titles too???!!!\n",
    "\n",
    "#########\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christophervolpacchio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christophervolpacchio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Webscraping, data managment and visualization libraries\n",
    "\n",
    "import pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier,\\\n",
    "AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,\\\n",
    "ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='scraping'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LOOP SCRAPER THROUGH SUBREDDITS WITH SEEDED DATAFRAME.\n",
    "\"\"\";\n",
    "# We are quickly reading in old scrapes to continue to feed our model.\n",
    "    # As we continue to scrape the training data will continue to grow.\n",
    "def reddit_scrape(subreddit):\n",
    "    api_url='https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "        'subreddit' : subreddit,\n",
    "    'size' : 1}\n",
    "    res = requests.get(api_url, params)\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    df = pd.DataFrame(posts)\n",
    "    count=0\n",
    "    while count < 21:\n",
    "        api_url='https://api.pushshift.io/reddit/search/submission'\n",
    "        params = {\n",
    "            'subreddit' : subreddit,\n",
    "            'size' : 100, # Current PushShift API limit.\n",
    "            'before' : min(df['created_utc'])}\n",
    "        res = requests.get(api_url, params)\n",
    "        print(res.status_code)\n",
    "        data = res.json()\n",
    "        # res.content shows key as 'data'.\n",
    "        posts = data['data']\n",
    "        df1 = pd.DataFrame(posts)\n",
    "        df = df.append(df1)\n",
    "        count += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "DF1 : 912 records\n",
      "DF2 : 2101 records\n"
     ]
    }
   ],
   "source": [
    "n1 = '\\n'\n",
    "arthur_df = reddit_scrape('Arthur')\n",
    "arthur_df.to_csv('./arthur.csv')\n",
    "attack_df = reddit_scrape('titanfolk')\n",
    "attack_df.to_csv('./titanfolk.csv')\n",
    "print(f\"DF1 : {len(arthur_df)} records{n1}DF2 : {len(attack_df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 912 entries, 0 to 11\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  912 non-null    object\n",
      " 1   title      912 non-null    object\n",
      " 2   selftext   910 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 28.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 920 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  920 non-null    object\n",
      " 1   title      920 non-null    object\n",
      " 2   selftext   914 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 28.8+ KB\n",
      "NoneNone\n"
     ]
    }
   ],
   "source": [
    "# We will slice our 'attack_df' to the approximate size of our Arthur data.\n",
    "\n",
    "attack_df = attack_df[:920]\n",
    "n1='\\n'\n",
    "print(f\"{arthur_df[['subreddit','title','selftext']].info()}{attack_df[['subreddit','title','selftext']].info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='cleaning'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 914 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  914 non-null    object\n",
      " 1   title      914 non-null    object\n",
      " 2   selftext   914 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 28.6+ KB\n"
     ]
    }
   ],
   "source": [
    "attack_df = attack_df[['subreddit','title','selftext']]\n",
    "attack_df1 = attack_df.copy()\n",
    "attack_df1.dropna(inplace = True)\n",
    "attack_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 910 entries, 0 to 11\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  910 non-null    object\n",
      " 1   title      910 non-null    object\n",
      " 2   selftext   910 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 28.4+ KB\n"
     ]
    }
   ],
   "source": [
    "arthur_df = arthur_df[['subreddit','title','selftext']]\n",
    "arthur_df1 = arthur_df.copy()\n",
    "arthur_df1.dropna(inplace = True)\n",
    "arthur_df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Importing pre-defined 'Stop Word' lists and using their union as a *super-set*. We will also be extracting program-specific text such as character names and locations to make classifier more reproducable.\n",
    "\n",
    "#### We are trying to gauge the general sentiment and tenor of each post to determine whether they would be TV–MA (Attack On Titan!) or TV–Y (Arthur) rated content.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='stops'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Count : 497, Unique Count : 378\n"
     ]
    }
   ],
   "source": [
    "stops0 = list(stopwords.words('english'));\n",
    "# This will return a tuple of index keys and strings that we will turn into a DataFrame.\n",
    "stops1 = pd.DataFrame(CountVectorizer(stop_words = 'english').get_stop_words());\n",
    "stops1['words']= stops1[0];\n",
    "stops1 = list(stops1['words'].items());\n",
    "\n",
    "# We are forced to use an extra step to extract information here because we are dealing with a from a 'Frozen Dictionary'.\n",
    "\n",
    "stop_words = []\n",
    "for i in stops1:\n",
    "    stop_words.append(i[1]);\n",
    "\n",
    "# Finalizing the creation of a more comprehensive list of stop words\n",
    "\n",
    "stop_words = stop_words + stops0;\n",
    "df = pd.DataFrame(stop_words);\n",
    "stop_words = list(df[0].unique());\n",
    "print(f\"String Count : {len(df[0])}, Unique Count : {len(df[0].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arthur</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>episode</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>know</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>episodes</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>think</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>remember</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>com</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kids</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>buster</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>really</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>watch</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>characters</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>francine</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>make</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>binky</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dw</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>season</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>muffy</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    1\n",
       "0       arthur  224\n",
       "1      episode  173\n",
       "2         like  124\n",
       "3         know   68\n",
       "4     episodes   66\n",
       "5        think   63\n",
       "6     remember   56\n",
       "7        https   54\n",
       "8          com   45\n",
       "9         kids   44\n",
       "10      buster   43\n",
       "11      really   43\n",
       "12       watch   43\n",
       "13  characters   37\n",
       "14    francine   35\n",
       "15        make   34\n",
       "16       binky   32\n",
       "17          dw   32\n",
       "18      season   32\n",
       "19       muffy   30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we will begin to look into program-specific stop words.\n",
    "\n",
    "arthur_df = arthur_df1\n",
    "cvec = CountVectorizer(lowercase = True, strip_accents = 'ascii', stop_words = list(stop_words))\n",
    "\n",
    "X = arthur_df['selftext']\n",
    "y = arthur_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train = cvec.fit_transform(X_train, y_train)\n",
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "top_words = {}\n",
    "\n",
    "# Loop through dataframe columns.\n",
    "for i in X_train_df.columns:\n",
    "    # Saving 'sum' of each column in dictionary.\n",
    "    top_words[i] =  X_train_df[i].sum()\n",
    "    \n",
    "# Casting top_words to a dataframe sorted by descending frequency.\n",
    "most_freq = pd.DataFrame(sorted(top_words.items(), key = lambda x: x[1], reverse = True))\n",
    "\n",
    "most_freq[:20]\n",
    "\n",
    "############################\n",
    "############################\n",
    "############################\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "#Can be good TO LOOK AT A DIST OF TERMS TO SEE WHERE IT IS WORTH CUTTING OFF\n",
    "#\"\"\"\n",
    "#top_words[i] = X_train_df[i].sum() \n",
    "\n",
    "#plt.bar(most_freq[0][:10], most_freq[1][:10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 Stop Words\n"
     ]
    }
   ],
   "source": [
    "arthur_df_stops = ['arthur','buster','muffy','francine', 'binky','dw']\n",
    "for i in arthur_df_stops:\n",
    "    stop_words.append(i);\n",
    "stop_words = set(stop_words)\n",
    "stop_words = list(stop_words)\n",
    "print(f'{len(stop_words)} Stop Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eren</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amp</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>historia</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>png</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deleted</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>titan</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>think</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poll</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>com</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>auto</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>webp</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>format</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>preview</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>redd</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>width</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>know</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>armin</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>world</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1\n",
       "0       eren  259\n",
       "1        amp  216\n",
       "2   historia  123\n",
       "3      https  123\n",
       "4        png  105\n",
       "5    deleted   95\n",
       "6      titan   84\n",
       "7      think   81\n",
       "8       poll   79\n",
       "9       like   76\n",
       "10       com   64\n",
       "11      auto   57\n",
       "12      webp   57\n",
       "13    format   56\n",
       "14   preview   56\n",
       "15      redd   56\n",
       "16     width   56\n",
       "17      know   55\n",
       "18     armin   50\n",
       "19     world   49"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_df = attack_df1[1:]\n",
    "cvec = CountVectorizer(lowercase = True, strip_accents = 'ascii', stop_words = list(stop_words))\n",
    "\n",
    "X = attack_df['selftext']\n",
    "y = attack_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train = cvec.fit_transform(X_train, y_train)\n",
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "top_words = {}\n",
    "\n",
    "# Loop through dataframe columns.\n",
    "for i in X_train_df.columns:\n",
    "    # Saving 'sum' of each column in dictionary.\n",
    "    top_words[i] =  X_train_df[i].sum()\n",
    "    \n",
    "# Casting top_words to a dataframe sorted by descending frequency.\n",
    "most_freq = pd.DataFrame(sorted(top_words.items(), key = lambda x: x[1], reverse = True))\n",
    "\n",
    "most_freq[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 Stop Words\n"
     ]
    }
   ],
   "source": [
    "attack_stops = ['https','historia','armin','titans','mikasa','eren']\n",
    "for i in attack_stops:\n",
    "    stop_words.append(i);\n",
    "stop_words = set(stop_words)\n",
    "stop_words = list(stop_words)\n",
    "print(f'{len(stop_words)} Stop Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Here we will run through our first model stripped-down model devoid of any hyperparameters, gridsearching, pipelining, etc.\n",
    "#### We will also establish the baseline accurracy rate that we are to measure against.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1823"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = arthur_df[['subreddit', 'title', 'selftext']].append(attack_df[['subreddit', 'title', 'selftext']])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Food question</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What episode did this happen on?</td>\n",
       "      <td>The one where Arthur roasts DW to a crisp aski...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Muffy and DW</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Luck episode</td>\n",
       "      <td>There was another episode where Arthur did not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Basketball question</td>\n",
       "      <td>Is there an actual episode where DW beats Arth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                             title  \\\n",
       "0          0                     Food question   \n",
       "0          0  What episode did this happen on?   \n",
       "1          0                      Muffy and DW   \n",
       "2          0                      Luck episode   \n",
       "3          0               Basketball question   \n",
       "\n",
       "                                            selftext  \n",
       "0                                          [deleted]  \n",
       "0  The one where Arthur roasts DW to a crisp aski...  \n",
       "1                                          [deleted]  \n",
       "2  There was another episode where Arthur did not...  \n",
       "3  Is there an actual episode where DW beats Arth...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'] = df['subreddit'].map({'Arthur' : 0, 'titanfolk' : 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1823 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  1823 non-null   int64 \n",
      " 1   title      1823 non-null   object\n",
      " 2   selftext   1823 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 57.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# There should NOT be any NA values at this point, however, we will run another clean to be safe.\n",
    "df.dropna(axis = 0, inplace = True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500823\n",
       "0    0.499177\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model must predict PG-rated content with > % accurracy of predicting a '1' in order to beat the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will 'tokenize' the words in each subreddit post and split our data to train our initial model.\n",
    "\n",
    "cvec = CountVectorizer(lowercase = True, strip_accents = 'ascii', stop_words = list(stop_words))\n",
    "\n",
    "X = df['selftext']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.33,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Logistic Regression classifier and measuring it's predictive accurracy.\n",
    "\n",
    "X_train = cvec.fit_transform(X_train, y_train)\n",
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "X_test = cvec.transform(X_test)\n",
    "X_test_df = pd.DataFrame(X_test.toarray(),\n",
    "                          columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model will predict a PG-rated program thread with 74.0% accurracy.\n",
      "Untrained data will predict with 69.4% accurracy.\n"
     ]
    }
   ],
   "source": [
    "# Instantiation, fitting and assessment of our model.\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_df, y_train)\n",
    "n1 = '\\n'\n",
    "print(f\"Our model will predict a PG-rated program thread with {round((lr.score(X_train_df, y_train))*100,1)}% accurracy.{n1}Untrained data will predict with {round((lr.score(X_test_df, y_test))*100,1)}% accurracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## We have <u>successfully</u> trained a model to have more predictive power than what we would infer from a breakdown of thread data alone.\n",
    "\n",
    "### We must now understand the *types* of errors that our model is returning as well as attempt to improve our predictive power through other models.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### I will first go up and *double* the record count to see if that will strengthen the model (whichever one that ends up being, anyway).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='moremodels'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Modeling\n",
    "\n",
    "#### Since we did *<u>not</u>* achieve 100% accuracy with out initial classification model attempt it is worth additional modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest with gridsearch hyperparams and pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6847139511542322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 15, 'max_features': 'auto', 'random_state': 11}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that \n",
    "#selects, at each split in the learning process, a random subset of the features. \n",
    "#This process is sometimes called the random subspace method. \n",
    "\"\"\"\n",
    "THIS WILL HOPEFULLY REDUCE THE HIGH VARIANCE USUALLY PRESENT TREE MODELS.\n",
    "\"\"\"\n",
    "rf = RandomForestClassifier() # This will ensure a new model each time.\n",
    "\n",
    "params = {'max_depth' : [5, 10, 15],#list(range(1,100,10)),\n",
    "         'max_features' : [None, 'auto'],\n",
    "          'random_state' : list(range(1,100,10))}\n",
    "#         'ccp_alpha' : list(range(0,5))}\n",
    "\n",
    "\n",
    "# We will gridsearch hyperparameters here to determine which parameter combination will yield us the most accurate model\n",
    "gs = GridSearchCV(rf, param_grid = params)\n",
    "\n",
    "# Our data is already split so we can simply plug in what we have already cleansed for assessment.\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7199017199017199"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9335548172757475"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####\n",
    "# enter md about CM's and the error types that we are looknig to minimize and why\n",
    "\n",
    "# Get predictions\n",
    "preds = gs.predict(X_test)\n",
    "# Save confusion matrix values\n",
    "tn,fp,fn,tp = confusion_matrix(y_test,preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', values_format='d');\n",
    "\n",
    "# Calculate the specificity (TN rate)\n",
    "tn/(tn+fp)\n",
    "tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9335548172757475"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## While our model does not display the most impressive overall accurracy, it still may prove to be very effective. \n",
    "\n",
    "#### Type 2 errors are the main source of our concern when we are trying to identify TV–MA content. A Type 2 error is also known as a  'false–negative' which in this case would mean that we predicted a the program content was *not* TV–MA rated though in actuality it was. Realizing this error type would be especially bad in this scenario because it would mean that we had flagged explicit content as safe for children of ages 5 and below to be watching. We would *much* rather prevent children from seeing program content that is not TV–MA (but we classify as such) than accidently showing them the oppossite. Thankfully through refinement of our model we can see that ~ 97% of TV–MA content is being properly identified. This number is referred to as our <u>True Positive Rate</u> and shows us the rate at which our model is finding unwanted content. Conversely, this would mean that ~ 3% of the time we are allowing some TV–MA content to pass through to a child viewer.\n",
    "\n",
    "#### This measurement displays the TV content can be especially jarring for children who are accustom to a certain brand of television and are highly attentive and impressionable when watching. We must keep in mind that television whether streamed through a cable box or through internet is still a popular medium for entertainment. Condisering that the first 5 years of a child's life contribute to ~ 90% of their brain development we want to limit harmful intake as much as possible\n",
    "\n",
    "---\n",
    "----\n",
    "\n",
    "#### In a last effort attempt to maximize our model's overall accurracy we will implement more modeling. In particular, an advanced boosting method known as XGBoost as well as pipelining will increase the number of models that our data will pass through and hopefully decrease our model variance.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose to use an ensemble model a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will ultimately use a bagging (bootstrapping & aggregating) method here to deploy\\\n",
    "#as many additional classifiers as we see fit (no pun intended here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d0724804efa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# with KNN there are two levels to go through i.e 2 dunders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\n\u001b[1;32m     67\u001b[0m                                                  sample_weight=sample_weight)\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclfs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_parallel_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    822\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1367\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just add in XGB and done!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "knn_pipe = Pipeline([('ss', StandardScaler(with_mean = False)),\n",
    "                    ('knn', KNeighborsClassifier())])\n",
    "\n",
    "vote = VotingClassifier([('ada', AdaBoostClassifier()),\n",
    "                        ('grad_boost', GradientBoostingClassifier()),\n",
    "                         ('XGB', xgb.XGBClassifier()),\n",
    "                        ('knn_pipe', knn_pipe)])\n",
    "\n",
    "# MUST USE DUNDERS TO DENOTE MODELS B/C OF SHARED PARAMS\n",
    "params = {\n",
    "    'ada__n_estimators': [100, 150, 200],\n",
    "    'knn_pipe__knn__n_neighbors' : [1,2,3,4,5,10,20],\n",
    "    'XGB__max_depth' : [10, 20, 30, 40],\n",
    "    'XGB__booster' : ['gbtree', 'gblinear', 'dart']\n",
    "}\n",
    "# with KNN there are two levels to go through i.e 2 dunders\n",
    "gs = GridSearchCV(vote, params, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "will update this list with character names, etc. to make classifier more reproducable and predictive on \n",
    "subject matter alone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation through Regular Expressions pattern.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_tokens_nopunct = [tokenizer.tokenize(str(x).lower()) for x in dff['selftext']]\n",
    "df_tokens_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = [token for token in df_tokens_nopunct if token not in stop_words]\n",
    "df_tokens\n",
    "####### DOES NOT YET CLEAN OUT STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[['selftext', 'subreddit']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a00226e3ac7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subreddit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dff' is not defined"
     ]
    }
   ],
   "source": [
    "dff.dropna(inplace = True)\n",
    "dff[['selftext', 'subreddit']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwirds HEREEEEEEEE\n",
    "#\n",
    "#\n",
    "#\n",
    "cvec = CountVectorizer(lowercase = True, strip_accents = 'ascii', stop_words = list(stop_words))\n",
    "\n",
    "X = dff['selftext']\n",
    "y = dff['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cvec.fit_transform(X_train, y_train)\n",
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "X_test = cvec.transform(X_test)\n",
    "X_test_df = pd.DataFrame(X_test.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {}\n",
    "\n",
    "# loop through columns\n",
    "for i in X_train_df.columns:\n",
    "    # save sum of each column in dictionary\n",
    "    top_words[i] =  X_train_df[i].sum()\n",
    "    \n",
    "# top_words to dataframe sorted by highest occurance\n",
    "most_freq = pd.DataFrame(sorted(top_words.items(), key = lambda x: x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words[i] = X_train_df[i].sum() \n",
    "\n",
    "plt.bar(most_freq[0][:10], most_freq[1][:10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_stops = ['Naruto', 'https', 'file', 'media', 'mp4', 'spongebob', 'delete']\n",
    "\n",
    "# [stop_words.append(i) for i in new_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get count of top-occurring words\n",
    "\n",
    "# # empty dictionary\n",
    "# top_words = {}\n",
    "\n",
    "# # loop through columns\n",
    "# for i in X_train_df.columns:\n",
    "#     # save sum of each column in dictionary\n",
    "#     top_words[i] =  X_train_df[i].sum()\n",
    "    \n",
    "# # top_words to dataframe sorted by highest occurance\n",
    "# most_freq = pd.DataFrame(sorted(top_words.items(), key = lambda x: x[1], reverse = True))\n",
    "\n",
    "# # View most frequent words dataframe\n",
    "# top_words[i] = X_train_df[i].sum() \n",
    "\n",
    "# plt.bar(most_freq[0][:10], most_freq[1][:10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must beat predictive power of our baseline which is currently the contribution of PG-rated shows to our target\n",
    "\n",
    "dff['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Need a model that will predict accurately over ~49% accurracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vectorize separate threads and remove char names and such\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.w3resource.com/python-exercises/string/python-data-type-string-exercise-12.php\n",
    "\n",
    "def word_count(field):\n",
    "    counts = dict()\n",
    "    for record in field:\n",
    "        words = record.split()\n",
    "        for word in words:\n",
    "            if word in counts:\n",
    "                counts[word] +=1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer does same thing?\n",
    "\n",
    "word_count(spongebob['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IN TERMS OF ACCURACY WE CARE MORE ABOUT FALSE NEG (TYPE II) THAN FALSE POS. B/C WE DO \n",
    "NOT WANT KIDS EXPOSED TO BAD STUFF\n",
    "\"\"\";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
